{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.9.0+cu102\n",
      "Torchvision Version:  0.10.0+cu102\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, io\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from random import random\n",
    "from PIL import Image\n",
    "import glob\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def set_seed(seed=1):\n",
    "    \"\"\"\n",
    "    Sets all random seeds.\n",
    "    :param seed: int\n",
    "        Seed value.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "torch.cuda.empty_cache()\n",
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"G:/tesina/Licencias/MIcroExpressions_Data\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 10\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MEDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, csv_file=\"\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.microExpresions = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.microExpresions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        images_path = os.path.join(self.root_dir, 'train', str(self.microExpresions.iloc[idx, 0]),\n",
    "                                   str(self.microExpresions.iloc[idx, 1]), str(self.microExpresions.iloc[idx, 2]))\n",
    "        array = []\n",
    "        count = 0;\n",
    "        for filename in glob.glob(images_path+'/*.jpg'):\n",
    "            im = Image.open(filename)\n",
    "            im = im.resize([224,224])\n",
    "            array.append(np.asarray(im))\n",
    "            count+=1;\n",
    "            if count >9:\n",
    "                break\n",
    "\n",
    "        if self.microExpresions.iloc[idx, 0] == \"Happiness\":\n",
    "            emotion = 0\n",
    "        elif self.microExpresions.iloc[idx, 0] == \"Disgust\":\n",
    "            emotion = 1\n",
    "        else:\n",
    "            emotion = 2\n",
    "        sample = {'images': array, 'emotion': emotion}\n",
    "\n",
    "        sample['images'] = torch.tensor(sample['images'])\n",
    "        sample['emotion'] = torch.tensor(sample['emotion'])\n",
    "\n",
    "\n",
    "        return sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        #for phase in ['train', 'val']:\n",
    "        for phase in ['train']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            #for inputs, labels in dataloaders[phase]:\n",
    "            for inputs in dataloaders:\n",
    "                inputs['images'] = inputs['images'].to(device).float()\n",
    "                inputs['emotion'] = inputs['emotion'].to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                #with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                print(inputs['images'].size())\n",
    "                inputs['images'] = inputs['images'].permute(0,4,1,2,3)\n",
    "                print(inputs['images'].size())\n",
    "\n",
    "                outputs = model(inputs['images'])\n",
    "                loss = criterion(outputs, inputs['emotion'])\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    #if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs['images'].size(0)\n",
    "                running_corrects += torch.sum(preds == inputs['emotion'])\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n",
    "\n",
    "            #print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format('train', epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            #if phase == 'val' and epoch_acc > best_acc:\n",
    "               # best_acc = epoch_acc\n",
    "               #best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            #if phase == 'val':\n",
    "               # val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MicroExpressionRecognition(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size = (5,5), stride=1,padding = 1,bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32, 32, kernel_size = (5,5), stride=1,padding = 1,bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32, 32, kernel_size = (5,5), stride=1,padding = 1,bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(22000, 128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.network}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        summary(self.network, (1, 48, 48))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class MicroExpressionRecognition3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size = (5,5,5), stride=(1,1,1),padding = (1,1,1),bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d([2,2,2]),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(788544, 128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.network}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        summary(self.network, (1, 48, 48))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#model_ft = MicroExpressionRecognition()\n",
    "model_ft = MicroExpressionRecognition3D()\n",
    "input_size = 224"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize([input_size,input_size]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.05),]), p=0.3),\n",
    "        #transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.03, 1), value=0, inplace=False)\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([input_size,input_size]),\n",
    "        #transforms.RandomGrayscale(p=0.1),\n",
    "        transforms.ToTensor()\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "#image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "#print(image_datasets['train'][1])\n",
    "\n",
    "image_datasets = MEDataset(root_dir='G:\\\\tesina\\\\Licencias\\\\MicroExpressions_Data2',\n",
    "                           transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                           csv_file='train_data.csv')\n",
    "# Create training and validation dataloaders\n",
    "#dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=x=='train', num_workers=10) for x in ['train', 'val']}\n",
    "dataloader = torch.utils.data.DataLoader(image_datasets, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "#dataloader_iter = iter(dataloader)\n",
    "# for step in range(len(dataloader)):\n",
    "#     data= next(dataloader_iter)\n",
    "#     print(f\"Feature batch shape: {data['images'][0].size()}\")\n",
    "#     print(f\"Labels batch shape: {data['emotion'][0]}\")\n",
    "#     new_im = data['images'][0][0]\n",
    "#     print(new_im)\n",
    "#     label = data['emotion']\n",
    "#     plt.imshow(new_im, cmap=\"gray\")\n",
    "#     plt.show()\n",
    "#train_features, train_labels = next(iter(dataloader))\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t network.0.weight\n",
      "\t network.0.bias\n",
      "\t network.5.weight\n",
      "\t network.5.bias\n",
      "\t network.7.weight\n",
      "\t network.7.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "#optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001, weight_decay = 0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tesina\\pruebas_1\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([6, 10, 224, 224, 3])\n",
      "torch.Size([6, 3, 10, 224, 224])\n",
      "train Loss: 1.2995 Acc: 0.2520\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n",
      "torch.Size([8, 10, 224, 224, 3])\n",
      "torch.Size([8, 3, 10, 224, 224])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Train and evaluate\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m model_ft, hist \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_ft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_ft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, dataloaders, criterion, optimizer, num_epochs)\u001B[0m\n\u001B[0;32m     22\u001B[0m running_corrects \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Iterate over data.\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m#for inputs, labels in dataloaders[phase]:\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs \u001B[38;5;129;01min\u001B[39;00m dataloaders:\n\u001B[0;32m     27\u001B[0m     inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m     28\u001B[0m     inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32mc:\\tesina\\pruebas_1\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    520\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[1;32m--> 521\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    524\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    525\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mc:\\tesina\\pruebas_1\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    560\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 561\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    562\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    563\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[1;32mc:\\tesina\\pruebas_1\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 44\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mc:\\tesina\\pruebas_1\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 44\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36mMEDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     40\u001B[0m     emotion \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m     41\u001B[0m sample \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m: array, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m: emotion}\n\u001B[1;32m---> 43\u001B[0m sample[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimages\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     44\u001B[0m sample[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(sample[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sample\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch.cuda.empty_cache()\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloader, criterion, optimizer_ft, num_epochs=num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!jupyter nbconvert  pruebas_1.ipynb --to html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}