{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.9.0+cu102\n",
      "Torchvision Version:  0.10.0+cu102\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, io\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from PIL import Image\n",
    "import glob\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def set_seed(seed=1):\n",
    "    \"\"\"\n",
    "    Sets all random seeds.\n",
    "    :param seed: int\n",
    "        Seed value.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "torch.cuda.empty_cache()\n",
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"G:/tesina/Licencias/MIcroExpressions_Data\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 16\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 10\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "set_seed(10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MEDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, csv_file=\"\", phase = ''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.microExpresions = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.microExpresions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        images_path = os.path.join(self.root_dir, self.phase, str(self.microExpresions.iloc[idx, 0]),\n",
    "                                   str(self.microExpresions.iloc[idx, 1]), str(self.microExpresions.iloc[idx, 2]))\n",
    "        array = []\n",
    "        for filename in glob.glob(images_path+'/*.jpg'):\n",
    "            im = Image.open(filename)\n",
    "            im = im.resize([224,224])\n",
    "            array.append(np.asarray(im))\n",
    "\n",
    "        if self.microExpresions.iloc[idx, 0] == \"happiness\":\n",
    "            emotion = 0\n",
    "        elif self.microExpresions.iloc[idx, 0] == \"disgust\":\n",
    "            emotion = 1\n",
    "        else:\n",
    "            emotion = 2\n",
    "        sample = {'images': array, 'emotion': emotion}\n",
    "\n",
    "        sample = self.transform(sample)\n",
    "        # sample['images'] = torch.tensor(sample['images'])\n",
    "        # sample['emotion'] = torch.tensor(sample['emotion'])\n",
    "\n",
    "\n",
    "        return sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        images, emotion = sample['images'], sample['emotion']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        return {'images': torch.tensor(images),\n",
    "                'emotion': torch.tensor(emotion)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class MakeGray(object):\n",
    "    def __call__(self, sample):\n",
    "        images, emotion = sample['images'], sample['emotion']\n",
    "\n",
    "        for im in images:\n",
    "            im = transforms.Grayscale(im)\n",
    "\n",
    "        return {'images': images, 'emotion': emotion}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        #for phase in ['train', 'val']:\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            #for inputs, labels in dataloaders[phase]:\n",
    "            for inputs in dataloaders[phase]:\n",
    "                inputs['images'] = inputs['images'].to(device).float()\n",
    "                inputs['emotion'] = inputs['emotion'].to(device).long()\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    inputs['images'] = inputs['images'].permute(0,4,1,2,3)\n",
    "                    inputs['images'].size()\n",
    "\n",
    "                    outputs = model(inputs['images'])\n",
    "                    print(outputs)\n",
    "                    print(inputs['emotion'])\n",
    "                    loss = criterion(outputs, inputs['emotion'])\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs['images'].size(0)\n",
    "                running_corrects += torch.sum(preds == inputs['emotion'])\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            #print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "               best_acc = epoch_acc\n",
    "               best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "               val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class MicroExpressionRecognition3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size = (3,3,3), stride=(1,1,1),padding = (1,1,1),bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout3d(0.2),\n",
    "            nn.MaxPool3d([1,2,2]),\n",
    "            nn.Conv3d(16, 16, kernel_size = (3,3,3), stride=(1,1,1),padding = (1,1,1),bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout3d(0.2),\n",
    "            nn.MaxPool3d([1,2,2]),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(150528, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.network}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        summary(self.network, (1, 48, 48))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#model_ft = MicroExpressionRecognition()\n",
    "model_ft = MicroExpressionRecognition3D()\n",
    "input_size = 224"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # transforms.Resize([input_size,input_size]),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        # transforms.RandomGrayscale(p=0.1),\n",
    "        # transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.05),]), p=0.3),\n",
    "        # transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n",
    "        # transforms.ToTensor(),\n",
    "        # transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.03, 1), value=0, inplace=False)\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ToTensor(),\n",
    "        MakeGray()\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        #transforms.Resize([input_size,input_size]),\n",
    "        #transforms.RandomGrayscale(p=0.1),\n",
    "        #transforms.ToTensor()\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: MEDataset(root_dir='G:\\\\tesina\\\\Licencias\\\\MicroExpressions_Data2',\n",
    "                           transform=data_transforms[x],\n",
    "                           csv_file=x+'_data.csv', phase=x) for x in ['train', 'val']}\n",
    "#print(image_datasets['train'][1])\n",
    "\n",
    "# image_datasets = MEDataset(root_dir='G:\\\\tesina\\\\Licencias\\\\MicroExpressions_Data2',\n",
    "#                            transform=transforms.Compose([transforms.ToTensor()]),\n",
    "#                            csv_file='train_data.csv', phase='train')\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=x=='train', num_workers=0) for x in ['train', 'val']}\n",
    "#dataloader = torch.utils.data.DataLoader(image_datasets, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "#dataloader_iter = iter(dataloader)\n",
    "# for step in range(len(dataloader)):\n",
    "#     data= next(dataloader_iter)\n",
    "#     print(f\"Feature batch shape: {data['images'][0].size()}\")\n",
    "#     print(f\"Labels batch shape: {data['emotion'][0]}\")\n",
    "#     new_im = data['images'][0][0]\n",
    "#     print(new_im)\n",
    "#     label = data['emotion']\n",
    "#     plt.imshow(new_im, cmap=\"gray\")\n",
    "#     plt.show()\n",
    "#train_features, train_labels = next(iter(dataloader))\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t network.0.weight\n",
      "\t network.0.bias\n",
      "\t network.4.weight\n",
      "\t network.4.bias\n",
      "\t network.10.weight\n",
      "\t network.10.bias\n",
      "\t network.13.weight\n",
      "\t network.13.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.0001, momentum=0.5)\n",
    "#optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001, weight_decay = 0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "tensor([[3.3333e-01, 3.3333e-01, 3.3333e-01],\n",
      "        [8.9412e-01, 5.2938e-02, 5.2938e-02],\n",
      "        [9.1079e-01, 4.4607e-02, 4.4607e-02],\n",
      "        [3.3333e-01, 3.3333e-01, 3.3333e-01],\n",
      "        [7.0283e-01, 1.4858e-01, 1.4858e-01],\n",
      "        [3.3333e-01, 3.3333e-01, 3.3333e-01],\n",
      "        [1.8213e-01, 6.3574e-01, 1.8213e-01],\n",
      "        [9.8765e-01, 6.1728e-03, 6.1728e-03],\n",
      "        [9.9553e-01, 2.2374e-03, 2.2374e-03],\n",
      "        [3.0864e-03, 9.9383e-01, 3.0864e-03],\n",
      "        [3.1418e-01, 6.2197e-01, 6.3850e-02],\n",
      "        [3.3333e-01, 3.3333e-01, 3.3333e-01],\n",
      "        [9.7396e-01, 1.3018e-02, 1.3018e-02],\n",
      "        [3.4295e-01, 3.2852e-01, 3.2852e-01],\n",
      "        [9.9998e-01, 1.0976e-05, 1.0976e-05],\n",
      "        [3.8678e-01, 3.0661e-01, 3.0661e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 0, 0, 1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1], device='cuda:0')\n",
      "tensor([[1.0000e+00, 2.1991e-08, 2.1991e-08],\n",
      "        [1.0000e+00, 8.1627e-11, 8.1627e-11],\n",
      "        [1.0000e+00, 1.5705e-06, 1.5705e-06],\n",
      "        [1.0000e+00, 7.9348e-09, 7.9348e-09],\n",
      "        [4.8649e-01, 2.5675e-01, 2.5675e-01],\n",
      "        [9.9991e-01, 4.6650e-05, 4.6650e-05],\n",
      "        [8.0521e-01, 6.9453e-02, 1.2534e-01],\n",
      "        [9.9995e-01, 2.7174e-05, 2.7174e-05],\n",
      "        [9.9902e-01, 5.8065e-04, 4.0091e-04],\n",
      "        [9.9994e-01, 3.1495e-05, 2.5400e-05],\n",
      "        [1.0000e+00, 5.7206e-13, 5.7206e-13],\n",
      "        [9.9999e-01, 6.3772e-06, 6.3772e-06],\n",
      "        [1.0000e+00, 4.9293e-08, 4.9293e-08],\n",
      "        [9.9926e-01, 3.6981e-04, 3.6981e-04],\n",
      "        [1.0000e+00, 3.3294e-12, 3.3294e-12],\n",
      "        [1.0000e+00, 2.3490e-07, 1.4161e-07]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 0, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 2, 2, 0, 1], device='cuda:0')\n",
      "tensor([[1.0000e+00, 1.2952e-07, 1.2952e-07],\n",
      "        [1.0000e+00, 2.4540e-15, 2.4540e-15],\n",
      "        [9.9999e-01, 3.5670e-06, 3.5670e-06],\n",
      "        [1.0000e+00, 6.3212e-13, 6.3212e-13],\n",
      "        [1.0000e+00, 2.0897e-14, 2.0897e-14],\n",
      "        [1.0000e+00, 4.0750e-17, 4.0750e-17],\n",
      "        [9.9999e-01, 4.6740e-06, 4.6740e-06],\n",
      "        [1.0000e+00, 1.6093e-08, 1.3500e-08],\n",
      "        [1.0000e+00, 4.2696e-08, 4.2696e-08],\n",
      "        [1.0000e+00, 8.5661e-09, 8.5661e-09],\n",
      "        [1.0000e+00, 2.2444e-06, 8.4483e-08],\n",
      "        [1.0000e+00, 2.8311e-08, 2.8311e-08],\n",
      "        [1.0000e+00, 7.2937e-11, 7.2937e-11],\n",
      "        [1.0000e+00, 9.2000e-18, 9.2000e-18],\n",
      "        [1.0000e+00, 8.2897e-11, 8.0733e-10],\n",
      "        [9.9992e-01, 5.5913e-05, 2.6434e-05]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 1, 1, 2, 1, 1, 1, 0, 0, 1, 2, 2, 1, 1, 0, 1], device='cuda:0')\n",
      "tensor([[9.9995e-01, 2.2875e-05, 2.2875e-05],\n",
      "        [1.0000e+00, 1.5423e-14, 1.5423e-14],\n",
      "        [1.0000e+00, 3.3165e-16, 3.3165e-16],\n",
      "        [1.0000e+00, 2.7983e-18, 2.7983e-18],\n",
      "        [1.0000e+00, 9.7158e-07, 9.7158e-07],\n",
      "        [1.0000e+00, 7.2287e-10, 7.2287e-10],\n",
      "        [1.0000e+00, 3.0230e-14, 3.0230e-14],\n",
      "        [1.0000e+00, 1.2930e-08, 1.2930e-08],\n",
      "        [1.0000e+00, 5.9705e-13, 5.9705e-13],\n",
      "        [9.9861e-01, 6.9688e-04, 6.9688e-04],\n",
      "        [1.0000e+00, 8.8812e-08, 1.0080e-08],\n",
      "        [1.0000e+00, 4.1386e-24, 4.1386e-24],\n",
      "        [1.0000e+00, 1.8196e-15, 1.8196e-15],\n",
      "        [8.0397e-01, 1.9134e-01, 4.6988e-03],\n",
      "        [1.0000e+00, 5.4098e-13, 5.4098e-13],\n",
      "        [1.0000e+00, 8.1977e-10, 1.1487e-09]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([2, 0, 1, 0, 1, 1, 1, 0, 0, 2, 2, 0, 1, 1, 2, 2], device='cuda:0')\n",
      "tensor([[1.0000e+00, 4.8246e-13, 1.4731e-13],\n",
      "        [1.0000e+00, 1.2392e-10, 4.6427e-12],\n",
      "        [1.0000e+00, 7.7557e-08, 7.7557e-08],\n",
      "        [6.9601e-01, 3.0399e-01, 1.8179e-06],\n",
      "        [1.0000e+00, 2.4437e-12, 2.4437e-12],\n",
      "        [1.0000e+00, 4.8273e-13, 4.8273e-13],\n",
      "        [1.0000e+00, 3.0016e-17, 1.4573e-17],\n",
      "        [1.0000e+00, 3.8922e-09, 1.1424e-10],\n",
      "        [3.9524e-02, 9.6046e-01, 2.0169e-05],\n",
      "        [1.0000e+00, 2.5225e-10, 2.5225e-10],\n",
      "        [1.0000e+00, 4.8188e-13, 6.1283e-14],\n",
      "        [9.9990e-01, 9.5590e-05, 2.2893e-06],\n",
      "        [1.0000e+00, 2.7605e-12, 2.7605e-12],\n",
      "        [1.0000e+00, 4.0703e-09, 4.0703e-09],\n",
      "        [1.0000e+00, 4.6772e-18, 4.6772e-18],\n",
      "        [1.0000e+00, 1.3274e-11, 1.3274e-11]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 0, 1, 1, 2, 2, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "tensor([[9.8324e-01, 1.6674e-02, 8.4095e-05],\n",
      "        [1.1906e-06, 1.0000e+00, 1.1906e-06],\n",
      "        [9.9942e-01, 5.5141e-04, 2.6955e-05],\n",
      "        [9.7310e-01, 2.5571e-02, 1.3320e-03],\n",
      "        [6.5199e-01, 3.4799e-01, 1.8497e-05],\n",
      "        [1.7296e-03, 9.9827e-01, 1.0475e-06],\n",
      "        [1.1588e-06, 1.0000e+00, 6.1356e-09],\n",
      "        [5.5201e-07, 1.0000e+00, 5.5201e-07],\n",
      "        [1.0075e-03, 9.9899e-01, 2.0116e-09],\n",
      "        [2.5209e-01, 7.4791e-01, 7.4330e-08],\n",
      "        [9.9999e-01, 3.7713e-06, 3.7713e-06],\n",
      "        [1.7810e-01, 8.2183e-01, 7.0857e-05],\n",
      "        [1.0000e+00, 7.7354e-07, 7.7354e-07],\n",
      "        [6.1700e-01, 3.8299e-01, 5.4528e-06],\n",
      "        [9.9979e-01, 1.0446e-04, 1.0446e-04],\n",
      "        [9.9997e-01, 1.5447e-05, 1.5447e-05]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 2, 2, 2, 1, 2, 2, 1, 0, 1, 1, 1, 1, 1, 2, 2], device='cuda:0')\n",
      "tensor([[2.1037e-20, 1.0000e+00, 1.1702e-13]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "train Loss: 1.2553 Acc: 0.2680\n",
      "tensor([[1.2001e-24, 1.0000e+00, 9.0026e-22],\n",
      "        [3.8636e-24, 1.0000e+00, 1.5408e-21],\n",
      "        [1.5242e-24, 1.0000e+00, 1.8905e-21],\n",
      "        [2.2127e-23, 1.0000e+00, 1.7363e-20],\n",
      "        [1.0720e-23, 1.0000e+00, 8.5637e-21],\n",
      "        [1.2112e-23, 1.0000e+00, 1.2954e-20],\n",
      "        [1.7994e-23, 1.0000e+00, 1.4510e-20],\n",
      "        [3.5510e-24, 1.0000e+00, 2.2817e-21],\n",
      "        [3.8692e-22, 1.0000e+00, 1.4020e-19],\n",
      "        [3.3998e-22, 1.0000e+00, 2.4512e-19],\n",
      "        [4.8020e-22, 1.0000e+00, 2.1778e-19],\n",
      "        [3.5773e-20, 1.0000e+00, 1.0633e-17],\n",
      "        [4.8982e-22, 1.0000e+00, 2.8171e-19],\n",
      "        [4.5124e-24, 1.0000e+00, 2.9678e-21],\n",
      "        [2.0498e-23, 1.0000e+00, 1.0278e-20],\n",
      "        [1.0429e-22, 1.0000e+00, 4.7845e-20]], device='cuda:0')\n",
      "tensor([0, 0, 0, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 0, 1, 1], device='cuda:0')\n",
      "tensor([[1.6026e-23, 1.0000e+00, 6.8053e-21],\n",
      "        [9.1372e-24, 1.0000e+00, 4.3666e-21],\n",
      "        [5.5458e-23, 1.0000e+00, 4.1658e-20],\n",
      "        [4.2536e-23, 1.0000e+00, 2.5807e-20],\n",
      "        [5.6375e-23, 1.0000e+00, 2.5244e-20],\n",
      "        [6.4729e-23, 1.0000e+00, 2.7372e-20],\n",
      "        [5.7946e-23, 1.0000e+00, 2.1814e-20],\n",
      "        [9.6227e-23, 1.0000e+00, 4.1216e-20]], device='cuda:0')\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "val Loss: 1.0098 Acc: 0.5417\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "tensor([[2.3155e-33, 1.0000e+00, 2.7561e-32],\n",
      "        [3.7614e-31, 1.0000e+00, 1.4772e-25],\n",
      "        [7.6937e-33, 1.0000e+00, 5.6965e-30],\n",
      "        [2.3612e-30, 1.0000e+00, 1.9654e-27],\n",
      "        [1.4987e-20, 1.0000e+00, 9.4070e-20],\n",
      "        [8.8208e-21, 1.0000e+00, 8.8208e-21],\n",
      "        [4.0732e-15, 1.0000e+00, 2.9049e-14],\n",
      "        [7.1910e-21, 1.0000e+00, 7.1910e-21],\n",
      "        [2.7998e-18, 1.0000e+00, 1.2278e-15],\n",
      "        [2.7941e-19, 1.0000e+00, 2.7941e-19],\n",
      "        [2.0525e-24, 1.0000e+00, 1.9655e-16],\n",
      "        [1.8833e-13, 1.0000e+00, 3.8535e-08],\n",
      "        [1.7945e-16, 1.0000e+00, 1.5291e-14],\n",
      "        [1.9698e-17, 1.0000e+00, 1.4969e-11],\n",
      "        [7.6091e-43, 1.0000e+00, 8.7301e-43],\n",
      "        [3.2948e-22, 1.0000e+00, 2.3203e-19]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 2, 2, 1, 1, 2, 2, 0, 2, 0, 1, 2, 1, 1, 0, 1], device='cuda:0')\n",
      "tensor([[3.3463e-42, 1.0000e+00, 2.1852e-29],\n",
      "        [7.1321e-21, 1.0000e+00, 7.1321e-21],\n",
      "        [2.1397e-29, 1.0000e+00, 1.8818e-27],\n",
      "        [9.8279e-31, 1.0000e+00, 1.1276e-27],\n",
      "        [1.8426e-33, 1.0000e+00, 6.3075e-28],\n",
      "        [5.7490e-28, 1.0000e+00, 1.0948e-27],\n",
      "        [2.5786e-32, 1.0000e+00, 3.0858e-25],\n",
      "        [4.1697e-32, 1.0000e+00, 1.0270e-25],\n",
      "        [2.5023e-25, 1.0000e+00, 4.1844e-21],\n",
      "        [1.2693e-26, 1.0000e+00, 1.1001e-21],\n",
      "        [2.3650e-29, 1.0000e+00, 9.6592e-21],\n",
      "        [2.0415e-27, 1.0000e+00, 9.9570e-21],\n",
      "        [4.1949e-37, 1.0000e+00, 2.3765e-24],\n",
      "        [1.0051e-24, 1.0000e+00, 3.2917e-21],\n",
      "        [3.1894e-35, 1.0000e+00, 5.8327e-30],\n",
      "        [8.0023e-37, 1.0000e+00, 6.3733e-30]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 1, 2, 2, 1, 1, 1, 0, 1, 0], device='cuda:0')\n",
      "tensor([[7.3699e-32, 1.0000e+00, 7.3699e-32],\n",
      "        [2.8026e-45, 1.0000e+00, 2.0510e-39],\n",
      "        [0.0000e+00, 1.0000e+00, 1.4013e-45],\n",
      "        [5.2557e-37, 1.0000e+00, 2.4134e-32],\n",
      "        [7.2245e-20, 1.0000e+00, 7.2245e-20],\n",
      "        [3.8530e-35, 1.0000e+00, 3.8530e-35],\n",
      "        [1.8525e-26, 1.0000e+00, 5.2212e-24],\n",
      "        [1.7174e-37, 1.0000e+00, 1.7174e-37],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
      "        [2.9427e-44, 1.0000e+00, 1.3120e-36],\n",
      "        [6.0953e-14, 1.0000e+00, 6.0953e-14],\n",
      "        [1.6327e-25, 1.0000e+00, 4.3584e-21],\n",
      "        [1.4013e-45, 1.0000e+00, 4.4018e-39],\n",
      "        [5.5741e-24, 1.0000e+00, 4.0490e-21],\n",
      "        [1.4457e-17, 1.0000e+00, 1.4457e-17],\n",
      "        [8.2918e-20, 1.0000e+00, 1.4456e-17]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 0, 2, 0, 0, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1], device='cuda:0')\n",
      "tensor([[4.3304e-40, 1.0000e+00, 1.9280e-34],\n",
      "        [2.8795e-36, 1.0000e+00, 2.7296e-33],\n",
      "        [1.4644e-27, 1.0000e+00, 5.8145e-21],\n",
      "        [8.7861e-43, 1.0000e+00, 6.9939e-42],\n",
      "        [4.9284e-22, 1.0000e+00, 2.1849e-20],\n",
      "        [2.0707e-39, 1.0000e+00, 1.0625e-31],\n",
      "        [8.5548e-29, 1.0000e+00, 7.1140e-21],\n",
      "        [2.4532e-31, 1.0000e+00, 7.8080e-30],\n",
      "        [5.9588e-18, 1.0000e+00, 2.1188e-16],\n",
      "        [9.3255e-39, 1.0000e+00, 1.7477e-33],\n",
      "        [7.9591e-23, 1.0000e+00, 8.9305e-22],\n",
      "        [1.9201e-26, 1.0000e+00, 5.0189e-26],\n",
      "        [2.8998e-30, 1.0000e+00, 8.0239e-25],\n",
      "        [4.7275e-27, 1.0000e+00, 1.3326e-20],\n",
      "        [1.1818e-40, 1.0000e+00, 2.3954e-26],\n",
      "        [2.6718e-21, 1.0000e+00, 3.8509e-19]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([2, 1, 1, 2, 0, 2, 0, 1, 1, 2, 1, 1, 1, 2, 1, 1], device='cuda:0')\n",
      "tensor([[6.8201e-27, 1.0000e+00, 4.8014e-24],\n",
      "        [4.4182e-40, 1.0000e+00, 5.7210e-36],\n",
      "        [1.7796e-43, 1.0000e+00, 3.0637e-36],\n",
      "        [7.4324e-30, 1.0000e+00, 1.2504e-24],\n",
      "        [2.2407e-42, 1.0000e+00, 1.5245e-41],\n",
      "        [3.9124e-20, 1.0000e+00, 1.5177e-17],\n",
      "        [5.0153e-30, 1.0000e+00, 1.7535e-26],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
      "        [1.2911e-30, 1.0000e+00, 1.2985e-25],\n",
      "        [1.3956e-33, 1.0000e+00, 2.1805e-21],\n",
      "        [1.0937e-07, 1.0000e+00, 1.0937e-07],\n",
      "        [0.0000e+00, 1.0000e+00, 3.9355e-39],\n",
      "        [1.4013e-45, 1.0000e+00, 1.7267e-35],\n",
      "        [1.2612e-44, 1.0000e+00, 7.5810e-43],\n",
      "        [2.2714e-39, 1.0000e+00, 1.7006e-35],\n",
      "        [4.6643e-33, 1.0000e+00, 3.3887e-29]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 0, 1, 2, 2, 1, 1, 0, 2, 0, 0, 1, 0, 1], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Train and evaluate\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m model_ft, hist \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_ft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_ft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, dataloaders, criterion, optimizer, num_epochs)\u001B[0m\n\u001B[0;32m     22\u001B[0m running_corrects \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Iterate over data.\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m#for inputs, labels in dataloaders[phase]:\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs \u001B[38;5;129;01min\u001B[39;00m dataloaders[phase]:\n\u001B[0;32m     27\u001B[0m     inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m     28\u001B[0m     inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mlong()\n",
      "File \u001B[1;32mc:\\tesina\\pruebas_1\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    520\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[1;32m--> 521\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    524\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    525\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mc:\\tesina\\pruebas_1\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    560\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 561\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    562\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    563\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[1;32mc:\\tesina\\pruebas_1\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 44\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mc:\\tesina\\pruebas_1\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 44\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36mMEDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     37\u001B[0m     emotion \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m     38\u001B[0m sample \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m: array, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m: emotion}\n\u001B[1;32m---> 40\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;66;03m# sample['images'] = torch.tensor(sample['images'])\u001B[39;00m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m# sample['emotion'] = torch.tensor(sample['emotion'])\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sample\n",
      "File \u001B[1;32mc:\\tesina\\pruebas_1\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:60\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     59\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 60\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, sample)\u001B[0m\n\u001B[0;32m      5\u001B[0m images, emotion \u001B[38;5;241m=\u001B[39m sample[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m], sample[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# swap color axis because\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# numpy image: H x W x C\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# torch image: C x H x W\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m     11\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor(emotion)}\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Train and evaluate\n",
    "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!jupyter nbconvert  pruebas_1.ipynb --to html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}